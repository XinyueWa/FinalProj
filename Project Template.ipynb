{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c5a136-e872-4846-9965-9ddc5251a0b3",
   "metadata": {},
   "source": [
    "# NYC Apartment Search\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1ogme9BJeHb2IZ6UREavUorF--nnxoWCYAAi8AZ4Q5jQ/edit?usp=sharing) and [grading rubric](https://docs.google.com/document/d/1XI9Yq_e-U-D3iH4jTPAtNteeP2Q9mtJ9NKbePWKeN_g/edit?usp=sharing)\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an idea of a possible approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf11fa0-4684-4f5e-8048-0f4cc5f4f243",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f38ff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geoalchemy2 in /Users/luyanni/anaconda3/lib/python3.11/site-packages (0.14.7)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4 in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from geoalchemy2) (1.4.39)\n",
      "Requirement already satisfied: packaging in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from geoalchemy2) (23.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from SQLAlchemy>=1.4->geoalchemy2) (2.0.1)\n",
      "Requirement already satisfied: geopandas in /Users/luyanni/anaconda3/lib/python3.11/site-packages (0.14.3)\n",
      "Requirement already satisfied: fiona>=1.8.21 in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from geopandas) (1.9.6)\n",
      "Requirement already satisfied: packaging in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from geopandas) (23.0)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from geopandas) (1.5.3)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from geopandas) (3.6.1)\n",
      "Requirement already satisfied: shapely>=1.8.0 in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from geopandas) (2.0.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (22.1.0)\n",
      "Requirement already satisfied: certifi in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (2024.2.2)\n",
      "Requirement already satisfied: click~=8.0 in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (8.0.4)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (1.1.1)\n",
      "Requirement already satisfied: cligj>=0.5 in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (0.7.2)\n",
      "Requirement already satisfied: six in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from fiona>=1.8.21->geopandas) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/luyanni/anaconda3/lib/python3.11/site-packages (from pandas>=1.4.0->geopandas) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install geoalchemy2\n",
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f675d4b-794e-407c-aac9-b85c4a3975d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All import statements needed for the project, for example:\n",
    "import json\n",
    "import pathlib\n",
    "import urllib.parse\n",
    "import os\n",
    "import geoalchemy2 as gdb\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import shapely\n",
    "import sqlalchemy as db\n",
    "from shapely.geometry import Point\n",
    "from sqlalchemy.orm import declarative_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70a62277-51cf-48a2-81d2-9b2127088a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where data files will be read from/written to - this should already exist\n",
    "DATA_DIR = pathlib.Path(\"/Users/catherinewang/Library/CloudStorage/GoogleDrive-xw2925@columbia.edu/My Drive/Final Project 4501/data\")\n",
    "ZIPCODE_DATA_FILE = DATA_DIR / \"nyc_zipcodes\" / \"nyc_zipcodes.shp\"\n",
    "ZILLOW_DATA_FILE = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "\n",
    "NYC_DATA_APP_TOKEN = \"1DrbcO2jXtETwL4T7Hm2ER2Lq\"\n",
    "BASE_NYC_DATA_URL = \"https://data.cityofnewyork.us/resource/\"\n",
    "NYC_DATA_311 = \"erm2-nwe9.geojson\"\n",
    "NYC_DATA_TREES = \"pi5s-9p35.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a25c5d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB_NAME = \"4501_Data_Analysis\"\n",
    "# DB_USER = \"user1\"\n",
    "# DB_URL = f\"postgres+psycopg2://{DB_USER}@localhost/{DB_NAME}\"\n",
    "# DB_SCHEMA_FILE = \"schema.sql\"\n",
    "# # directory where DB queries for Part 3 will be saved\n",
    "# QUERY_DIR = pathlib.Path(\"queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd67cca9-ec72-44e3-83b8-b65f1ed5bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY & DATA_DIR exists\n",
    "if not DATA_DIR.exists():\n",
    "    DATA_DIR.mkdir()\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52476a07-9bf2-4b7a-8cb7-93648bb4d303",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63b18f12-c0ce-4b9c-adc1-805703edc575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nyc_geojson_data(url, force=False):\n",
    "    # Parse the given URL\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    url_path = parsed_url.path.strip(\"/\")\n",
    "\n",
    "    # Construct the filename based on the URL path\n",
    "    filename = DATA_DIR / url_path\n",
    "\n",
    "    # Check if the directory exists, and create it if it does not\n",
    "    if not filename.parent.exists():\n",
    "        os.makedirs(filename.parent, exist_ok=True)\n",
    "\n",
    "    # Download the data if force is True or if the file does not exist\n",
    "    if force or not filename.exists():\n",
    "        print(f\"Downloading {url} to {filename}...\")\n",
    "        try:\n",
    "            # Request the data from the URL\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raises a HTTPError if the response was an error\n",
    "\n",
    "            # Save the data to the file\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Done downloading {url}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {url}. Error: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"File already exists at {filename}.\")\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee245240-2fbb-45b8-9a92-4e2368f62c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zipcodes(ZIPCODE_DATA_FILE):\n",
    "    # Load the shapefile\n",
    "    gdf = gpd.read_file(ZIPCODE_DATA_FILE)\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    gdf = gdf.dropna()\n",
    "\n",
    "    # List of columns to keep\n",
    "    columns_to_keep = ['ZIPCODE', 'geometry']\n",
    "    gdf = gdf[columns_to_keep]\n",
    "\n",
    "    # Remove duplicate ZIP codes\n",
    "    gdf = gdf.drop_duplicates(subset='ZIPCODE')\n",
    "\n",
    "    # Filter out ZIP codes that are not in NYC\n",
    "    valid_nyc_zip_codes = (\n",
    "        [f\"{z:05d}\" for z in range(10001, 10293)] +  # Manhattan\n",
    "        [f\"{z:05d}\" for z in range(10451, 10476)] +  # Bronx\n",
    "        [f\"{z:05d}\" for z in range(11201, 11257)] +  # Brooklyn\n",
    "        [f\"{z:05d}\" for z in range(11001, 11698)] +  # Queens\n",
    "        [f\"{z:05d}\" for z in range(10301, 10315)]    # Staten Island\n",
    "    )\n",
    "    gdf = gdf[gdf['ZIPCODE'].isin(valid_nyc_zip_codes)]\n",
    "\n",
    "    # Cast columns to appropriate types\n",
    "    gdf['ZIPCODE'] = gdf['ZIPCODE'].astype(str)\n",
    "\n",
    "    # Save the cleaned GeoDataFrame back to a new shapefile\n",
    "    cleaned_file_path = ZIPCODE_DATA_FILE.parent / \"nyc_zipcodes_cleaned.shp\"\n",
    "    gdf.to_file(cleaned_file_path)\n",
    "\n",
    "    return cleaned_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eed2a5a9-1027-4c41-bbb5-039c32ce7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_311_data():\n",
    "    limit = 1000\n",
    "    offset = 0\n",
    "    all_gdf = gpd.GeoDataFrame()\n",
    "\n",
    "    while True:\n",
    "        query = f\"?$where=created_date >= '2022-02-01T00:00:00' AND created_date <= '2024-02-29T00:00:00' AND latitude IS NOT NULL&$limit={limit}&$offset={offset}&$$app_token={NYC_DATA_APP_TOKEN}\"\n",
    "        full_url = f\"{BASE_NYC_DATA_URL}{NYC_DATA_311}{query}\"\n",
    "\n",
    "        filename = download_nyc_geojson_data(full_url, True)\n",
    "        if filename is None:\n",
    "            print(f\"Failed to download data for offset {offset}.\")\n",
    "            break\n",
    "\n",
    "        gdf = gpd.read_file(filename)\n",
    "\n",
    "        if gdf.empty:\n",
    "            print(\"No more data to process.\")\n",
    "            break\n",
    "\n",
    "        gdf.columns = [col.lower().replace(' ', '_') for col in gdf.columns]\n",
    "\n",
    "        necessary_columns = ['unique_key', 'created_date', 'incident_zip', 'complaint_type','city', 'geometry']\n",
    "\n",
    "        gdf = gdf[necessary_columns].dropna(subset=['incident_zip', 'city'])\n",
    "\n",
    "        gdf['created_date'] = pd.to_datetime(gdf['created_date'])\n",
    "\n",
    "        all_gdf = pd.concat([all_gdf, gdf], ignore_index=True)\n",
    "\n",
    "        offset += limit\n",
    "        break\n",
    "    if not all_gdf.empty:\n",
    "        all_gdf.set_crs(epsg=4326, inplace=True)\n",
    "        geojson_path = DATA_DIR / 'nyc_311_data_cleaned.geojson'\n",
    "        all_gdf.to_file(geojson_path, driver='GeoJSON')\n",
    "        print(f\"Data saved to {geojson_path}\")\n",
    "\n",
    "    return all_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78c84623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://data.cityofnewyork.us/resource/erm2-nwe9.geojson?$where=created_date >= '2022-02-01T00:00:00' AND created_date <= '2024-02-29T00:00:00' AND latitude IS NOT NULL&$limit=1000&$offset=0&$$app_token=1DrbcO2jXtETwL4T7Hm2ER2Lq to /Users/catherinewang/Library/CloudStorage/GoogleDrive-xw2925@columbia.edu/My Drive/Final Project 4501/data/resource/erm2-nwe9.geojson...\n",
      "Done downloading https://data.cityofnewyork.us/resource/erm2-nwe9.geojson?$where=created_date >= '2022-02-01T00:00:00' AND created_date <= '2024-02-29T00:00:00' AND latitude IS NOT NULL&$limit=1000&$offset=0&$$app_token=1DrbcO2jXtETwL4T7Hm2ER2Lq.\n",
      "Data saved to /Users/catherinewang/Library/CloudStorage/GoogleDrive-xw2925@columbia.edu/My Drive/Final Project 4501/data/nyc_311_data_cleaned.geojson\n"
     ]
    }
   ],
   "source": [
    "clean_311_nyc_data = download_and_clean_311_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39c4b1bc-c841-4b87-8301-1dc2cafeccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_tree_data():\n",
    "    limit = 1000\n",
    "    offset = 0\n",
    "    all_trees_gdf = gpd.GeoDataFrame(columns=['tree_id', 'zipcode', 'geometry', 'status', 'health', 'spc_latin', 'zip_city'])  # Initialize with columns\n",
    "    all_trees_gdf.set_crs(epsg=4326, inplace=True)  # Set CRS to WGS 84 from the start\n",
    "\n",
    "    while True:\n",
    "        query = f\"?$where=latitude IS NOT NULL&$limit={limit}&$offset={offset}&$$app_token={NYC_DATA_APP_TOKEN}\"\n",
    "        full_url = f\"https://data.cityofnewyork.us/resource/uvpi-gqnh.geojson{query}\"\n",
    "        filename = download_nyc_geojson_data(full_url, True)  # Assume this function returns the filepath correctly\n",
    "\n",
    "        if not filename:\n",
    "            break  # Exit the loop if no filename is returned\n",
    "\n",
    "        gdf_batch = gpd.read_file(filename)\n",
    "\n",
    "        if gdf_batch.empty:\n",
    "            print(\"No more data to process.\")\n",
    "            break\n",
    "\n",
    "        gdf_batch.columns = [col.lower().replace(' ', '_') for col in gdf_batch.columns]\n",
    "\n",
    "        # Filter for necessary columns, including latitude and longitude for geometry creation\n",
    "        columns_to_keep = ['tree_id', 'zipcode', 'latitude', 'longitude', 'status', 'health', 'spc_latin', 'zip_city']\n",
    "        gdf_batch = gdf_batch[columns_to_keep].dropna(subset=['zipcode', 'zip_city'])\n",
    "\n",
    "        # Convert latitude and longitude to float\n",
    "        gdf_batch['latitude'] = gdf_batch['latitude'].astype(float)\n",
    "        gdf_batch['longitude'] = gdf_batch['longitude'].astype(float)\n",
    "\n",
    "        # Create geometry column from latitude and longitude\n",
    "        gdf_batch['geometry'] = [Point(xy) for xy in zip(gdf_batch.longitude, gdf_batch.latitude)]\n",
    "        # Drop the latitude and longitude columns as they are no longer needed\n",
    "        gdf_batch.drop(columns=['latitude', 'longitude'], inplace=True)\n",
    "        # Create a GeoDataFrame with the geometry column\n",
    "        gdf_batch = gpd.GeoDataFrame(gdf_batch, geometry='geometry')\n",
    "        gdf_batch.set_crs(epsg=4326, inplace=True)  # Ensure the GeoDataFrame is in WGS 84 CRS\n",
    "\n",
    "        # Concatenate with the main GeoDataFrame\n",
    "        all_trees_gdf = pd.concat([all_trees_gdf, gdf_batch], ignore_index=True)\n",
    "\n",
    "        offset += limit  # Increment the offset for the next batch\n",
    "        break\n",
    "    if not all_trees_gdf.empty:\n",
    "        # Save the cleaned GeoDataFrame to a GeoJSON file, now that geometry is used\n",
    "        cleaned_filepath = DATA_DIR / 'nyc_trees_data_cleaned.geojson'\n",
    "        all_trees_gdf.to_file(cleaned_filepath, driver='GeoJSON')\n",
    "        print(f\"Cleaned tree data saved to {cleaned_filepath}\")\n",
    "\n",
    "    return all_trees_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93789da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://data.cityofnewyork.us/resource/uvpi-gqnh.geojson?$where=latitude IS NOT NULL&$limit=1000&$offset=0&$$app_token=1DrbcO2jXtETwL4T7Hm2ER2Lq to /Users/catherinewang/Library/CloudStorage/GoogleDrive-xw2925@columbia.edu/My Drive/Final Project 4501/data/resource/uvpi-gqnh.geojson...\n",
      "Done downloading https://data.cityofnewyork.us/resource/uvpi-gqnh.geojson?$where=latitude IS NOT NULL&$limit=1000&$offset=0&$$app_token=1DrbcO2jXtETwL4T7Hm2ER2Lq.\n",
      "Cleaned tree data saved to /Users/catherinewang/Library/CloudStorage/GoogleDrive-xw2925@columbia.edu/My Drive/Final Project 4501/data/nyc_trees_data_cleaned.geojson\n"
     ]
    }
   ],
   "source": [
    "clean_tree_nyc_data = download_and_clean_tree_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "747ff49f-a18b-4fc0-8da6-6834a10d11ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zillow_data(NYC_Cities):\n",
    "    # Load the data into a DataFrame\n",
    "    zillow_rent_df = pd.read_csv(ZILLOW_DATA_FILE)\n",
    "    \n",
    "    # Assuming the date columns are in a format like 'YYYY-MM-DD'\n",
    "    # First, identify all columns that are not date columns but need to be kept (e.g., 'RegionName', 'City')\n",
    "    non_date_columns = ['RegionName', 'City']\n",
    "\n",
    "    # Dynamically generate a list of date columns to keep, based on the specified date range\n",
    "    # Convert all column names to datetime where possible; this will yield NaT for non-date columns\n",
    "    date_columns = pd.to_datetime(zillow_rent_df.columns, errors='coerce', format='%Y-%m-%d')\n",
    "\n",
    "    # Identify columns within the specified date range\n",
    "    start_date = pd.to_datetime('2022-02-01')\n",
    "    end_date = pd.to_datetime('2024-01-31')\n",
    "    date_columns_to_keep = zillow_rent_df.columns[(date_columns >= start_date) & (date_columns <= end_date)]\n",
    "\n",
    "    # Combine the non-date columns to keep with the date columns identified in the range\n",
    "    columns_to_keep = non_date_columns + date_columns_to_keep.tolist()\n",
    "\n",
    "    # Filter the DataFrame to include only the identified columns\n",
    "    zillow_rent_df = zillow_rent_df[columns_to_keep]\n",
    "\n",
    "    # Filter rows where 'City' is in the NYC_Cities list\n",
    "    zillow_rent_df = zillow_rent_df[zillow_rent_df['City'].str.lower().isin([city.lower() for city in NYC_Cities])]\n",
    "\n",
    "    return zillow_rent_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345ebc2c-14f1-490c-9857-11f1e332e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    geodf_zipcode_data = load_and_clean_zipcodes(ZIPCODE_DATA_FILE)\n",
    "    geodf_311_data = download_and_clean_311_data()\n",
    "    geodf_tree_data = download_and_clean_tree_data()\n",
    "    df_zillow_data = load_and_clean_zillow_data()\n",
    "    return (\n",
    "        geodf_zipcode_data,\n",
    "        geodf_311_data,\n",
    "        geodf_tree_data,\n",
    "        df_zillow_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2768bc8-4130-4298-be28-13d4b250a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_zipcode_data, geodf_311_data, geodf_tree_data, df_zillow_data = load_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad8bbc-bf91-457e-97db-a945fabeee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic info about each dataframe\n",
    "geodf_zipcode_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68f4be-f365-46c1-91a1-ab75deb75ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 5 entries about each dataframe\n",
    "geodf_zipcode_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a803b68-2f07-44b8-8b24-d4f16c9e03fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_311_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14705df9-ea77-4d57-ac8e-1845f80a216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_311_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6006cd2-3a00-4660-8d2a-a660b9bfd91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_tree_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f880ef-c5fc-4159-8174-21ccd44f492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_tree_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59724f74-5f1e-435c-b843-f381a875dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zillow_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ae5d9-9768-4590-a2f2-dd63b07dd712",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zillow_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685942c-26dc-40db-84c2-a71aa3340806",
   "metadata": {},
   "source": [
    "## Part 2: Storing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349fbdd-67d0-40a4-97a0-d9b8c8ec8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_new_postgis_database(username, db_name):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ed80d-7b60-484f-a123-23b673b0f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_new_postgis_database(DB_USER, DB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a251c-f337-4b24-bb41-96ee4621a9bd",
   "metadata": {},
   "source": [
    "### Creating Tables\n",
    "\n",
    "\n",
    "These are just a couple of options to creating your tables; you can use one or the other, a different method, or a combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d72390-3c2d-4856-82c0-3284e8ccb24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DB_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac07405-dc2b-47af-9dad-6a9b94d2b34c",
   "metadata": {},
   "source": [
    "#### Option 1: SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d0cc6-74b3-4d35-a454-57f647c9f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using SQL (as opposed to SQLAlchemy), define the SQL statements to create your 4 tables.\n",
    "# You may be creating more tables depending on how you're setting up your constraints/relationships\n",
    "# or if you're completing the extra credit.\n",
    "ZIPCODE_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "ZIPCODE_G\n",
    "\n",
    "NYC_311_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "NYC_TREE_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "ZILLOW_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d86f6-ff6e-4bb8-8fa2-df0d4282e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DB_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(ZIPCODE_SCHEMA)\n",
    "    f.write(NYC_311_SCHEMA)\n",
    "    f.write(NYC_TREE_SCHEMA)\n",
    "    f.write(ZILLOW_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eacd37-4fd7-4768-b689-88b07d5c234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using SQL (as opposed to SQLAlchemy), execute the schema files to create tables\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232a2d89-b276-4d44-a0ef-3631eb686e84",
   "metadata": {},
   "source": [
    "#### Option 2: SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c42e1-4ad4-4c43-a2ba-1dbcac1de557",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class Tree(Base):\n",
    "    __tablename__ = \"trees\"\n",
    "\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a9c3d-e6d6-4e01-8247-e9d465381ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88a50c-9528-4a5c-9a52-b96781ee8985",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "These are just a couple of options to write data to your tables; you can use one or the other, a different method, or a combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c66af67-afb8-4f0d-bb57-552972f8e4b8",
   "metadata": {},
   "source": [
    "#### Option 1: SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e37800-cd95-44b5-9c21-eb7ac2b2e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(tablename_to_dataframe):\n",
    "    # write INSERT statements or use pandas/geopandas to write SQL\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f042f5-8270-477d-929a-872f7d9a0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename_to_dataframe = {\n",
    "    \"zipcodes\": geodf_zipcode_data,\n",
    "    \"complaints\": geodf_311_data,\n",
    "    \"trees\": geodf_tree_data,\n",
    "    \"rents\": df_zillow_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d052c50-1e43-4356-bcac-4f5abc7e714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(tablename_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4708cb4-d034-43b6-955b-a21d0eab74d4",
   "metadata": {},
   "source": [
    "#### Option 2: SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ddbad-3b11-47a2-9245-935f482fa7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Session = db.orm.sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b21c6-59d9-4bae-8c33-ab4b49ff2b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in geodf_tree_data.iterrows():\n",
    "    tree = Tree(...)\n",
    "    session.add(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4bfb9-4fbc-45fe-8a98-a760a22234f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb63b553-0c64-4da8-9fc7-41555d89d853",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7e12b-e251-4f08-8dc5-601db30c2089",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce8548-4aba-4bf9-992c-dedd0f249db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605e6f3-ec42-4a8b-833c-5138c14b678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = QUERY_DIR / \"FILL_ME_IN\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "FILL_ME_IN\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce044adf-ecdf-4237-9b20-b7cdaaab0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(db.text(QUERY_1))\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7b2c3d-8961-4c7e-8eb1-fc973d0ab9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75223ce5-6ab5-4613-b6af-fa8e33bcc7d5",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21fcfed-ddbb-4908-a60e-ed7cbc6d5b00",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e2cde-e43b-407b-ab93-ff85a2dba469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80755f-d1e1-4e53-8ef8-f5295c59a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query your database for the data needed.\n",
    "    # You can put the data queried into a pandas/geopandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2632a-b516-4a6e-8b67-97116ab6fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
